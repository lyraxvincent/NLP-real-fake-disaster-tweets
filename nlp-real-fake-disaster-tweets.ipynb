{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":33,"outputs":[{"output_type":"stream","text":"/kaggle/input/nlp-getting-started/sample_submission.csv\n/kaggle/input/nlp-getting-started/test.csv\n/kaggle/input/nlp-getting-started/train.csv\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Some more imports\n##\nimport warnings\nwarnings.filterwarnings(\"ignore\")","execution_count":80,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\nsample_submission","execution_count":34,"outputs":[{"output_type":"execute_result","execution_count":34,"data":{"text/plain":"         id  target\n0         0       0\n1         2       0\n2         3       0\n3         9       0\n4        11       0\n...     ...     ...\n3258  10861       0\n3259  10865       0\n3260  10868       0\n3261  10874       0\n3262  10875       0\n\n[3263 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3258</th>\n      <td>10861</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3259</th>\n      <td>10865</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3260</th>\n      <td>10868</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>10875</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":35,"outputs":[{"output_type":"execute_result","execution_count":35,"data":{"text/plain":"   id keyword location                                               text  \\\n0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n\n   target  \n0       1  \n1       1  \n2       1  \n3       1  \n4       1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Our Deeds are the Reason of this #earthquake M...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Forest fire near La Ronge Sask. Canada</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>5</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>All residents asked to 'shelter in place' are ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>13,000 people receive #wildfires evacuation or...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.head()","execution_count":36,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"   id keyword location                                               text\n0   0     NaN      NaN                 Just happened a terrible car crash\n1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>keyword</th>\n      <th>location</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Just happened a terrible car crash</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Heard about #earthquake is different cities, s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>there is a forest fire at spot pond, geese are...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Apocalypse lighting. #Spokane #wildfires</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['text'].sample(30)","execution_count":37,"outputs":[{"output_type":"execute_result","execution_count":37,"data":{"text/plain":"2987    What This Man Did To Save A Drowning Squirrel ...\n4672    Punjab government flood relief platform: http:...\n3491    #cum explosion!\\n\\n@begforcum \\n@allday_cumsho...\n3513    #Eyewitness media is actively embraced by #UK ...\n3688                     Fatality https://t.co/GF5qjGoyCi\n5417    I feel like I should be panicking more as Idk ...\n3056    #USGS M 1.4 - 4km E of Interlaken California: ...\n1551    Downtown Emergency Service Center is hiring! #...\n6196    @LifeAintFairKid if I did I'd smoke you up brooo!\n6940    If you have trouble getting motivated remember...\n4872    Not only are you a mass murderer but at a movi...\n3274    [Question] Is anybody else having this problem...\n2147    Weyburn Police Warn Public after Fentanyl Deat...\n616     A Tale of Two Pox - Body Horrors http://t.co/W...\n4997    13 reasons why we love women in the military  ...\n3745       My asshole is on fire  https://t.co/Y3FO0gHg8t\n397     Mourning notices for stabbing arson victims st...\n7407    Prince Phillip said of the numbers of those mu...\n5917    @GodOf_Mischief_ -of Loki's daggers she pulled...\n936     'WeÛªre blown away by this extension. Nothing...\n2378    This is how we know #AllLivesMatter people are...\n2800           Û¢i'm the architect of my own disasterÛ¢\n802     LIKE I SWEAR THE SECRET WE'LL UNCOVER IS THE O...\n3039    Raffi_RC: RT SustainOurEarth: Oklahoma Acts to...\n517     #WeLoveLA #NHLDucks Avalanche Defense: How The...\n4198    Freak accident? Sure. Looking for someone to b...\n385     Los Angeles Times: Arson suspect linked to 30 ...\n2236    If you're in search of powerful content to imp...\n3787    @KapoKekito on northgate by the taco truck tha...\n2152    @HighQualityBird a reverse situation (lol I do...\nName: text, dtype: object"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Some EDA"},{"metadata":{},"cell_type":"markdown","source":"Missing data"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.isnull().sum()","execution_count":38,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"id             0\nkeyword       61\nlocation    2533\ntext           0\ntarget         0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['keyword'].value_counts()[:10]","execution_count":39,"outputs":[{"output_type":"execute_result","execution_count":39,"data":{"text/plain":"fatalities     45\ndeluge         42\narmageddon     42\ndamage         41\nsinking        41\nharm           41\nbody%20bags    41\nevacuate       40\ntwister        40\nfear           40\nName: keyword, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# A list to randomly fill the missing keywords\n##\nkwords = ['fatalities', 'deluge', 'armageddon', 'damage', 'sinking', 'harm', 'body%20bags', \n          'evacuate', 'twister', 'fear']","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(train['keyword'][0]) == float","execution_count":74,"outputs":[{"output_type":"execute_result","execution_count":74,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Impute rows with missing keyword\n##\nfor i, kword in enumerate(train['keyword']):\n    if type(kword) == float: # Note that type of nan is float\n        train['keyword'][i] = np.random.choice(kwords)\n    else:\n        pass\n\n# Checking to see that the rows have been filled\ntrain['keyword'].isnull().values.any()","execution_count":75,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n","name":"stderr"},{"output_type":"execute_result","execution_count":75,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preview missing  keywords in test data\n##\ntest.isnull().sum()","execution_count":78,"outputs":[{"output_type":"execute_result","execution_count":78,"data":{"text/plain":"id             0\nkeyword       26\nlocation    1105\ntext           0\ndtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Same treatment to the test data\n##\nfor i, kword in enumerate(test['keyword']):\n    if type(kword) == float: # Note that type of nan is float\n        test['keyword'][i] = np.random.choice(kwords)\n    else:\n        pass\n\n# Checking to see that the rows have been filled\ntest['keyword'].isnull().values.any()","execution_count":79,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  \"\"\"\n","name":"stderr"},{"output_type":"execute_result","execution_count":79,"data":{"text/plain":"False"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Text Processing"},{"metadata":{},"cell_type":"markdown","source":"**Text processing require:**  \n- remove punctuation  \n- remove numbers  \n- to lowercase  \n- remove stopwords  \n- lemmatize  \n- remove non-english words  \n\n**NOTE** : The order of the steps matter. The text processing function should execute steps in this outlined order."},{"metadata":{"trusted":true},"cell_type":"code","source":"# A function to process text as outlined above\n\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nwords = set(nltk.corpus.words.words())\n\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n\ndef text_process(text):\n    cln_text = [c for c in text if c not in string.punctuation] # punctuation removal\n    cln_text = ''.join(cln_text)\n    cln_text = [c for c in cln_text if not c.isdigit()] # number removal\n    cln_text = ''.join(cln_text)\n    cln_text = cln_text.lower() # lowercase\n    cln_text = [word for word in cln_text.split() if word not in stopwords.words('english')] # stopwords removal\n    cln_text = ' '.join(cln_text)\n    cln_text = [lemmatizer.lemmatize(word) for word in nltk.word_tokenize(cln_text)] # lemmatization\n    cln_text = ' '.join(cln_text)\n    cln_text = [word for word in nltk.wordpunct_tokenize(cln_text) if word in words or not word.isalpha()] # removal of non-english words\n    return cln_text","execution_count":82,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Example of how the function works on a single text\n##\ntext_process(train['text'][0])","execution_count":88,"outputs":[{"output_type":"execute_result","execution_count":88,"data":{"text/plain":"['deed', 'reason', 'earthquake', 'may', 'forgive', 'u']"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# BOW model\n##\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nbow_transformer = CountVectorizer(analyzer=text_process)\nbow_transformer.fit(train['text'])","execution_count":89,"outputs":[{"output_type":"execute_result","execution_count":89,"data":{"text/plain":"CountVectorizer(analyzer=<function text_process at 0x7f25fb8ea1e0>,\n                binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Number of BOW vocab words\n##\nlen(bow_transformer.vocabulary_)","execution_count":90,"outputs":[{"output_type":"execute_result","execution_count":90,"data":{"text/plain":"6702"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming a single text\n##\nprint(bow_transformer.transform([train['text'][1230]]))","execution_count":91,"outputs":[{"output_type":"stream","text":"  (0, 325)\t1\n  (0, 1048)\t1\n  (0, 2099)\t1\n  (0, 2248)\t1\n  (0, 2698)\t1\n  (0, 3017)\t1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting feature names\n##\nbow_transformer.get_feature_names()[1048]","execution_count":92,"outputs":[{"output_type":"execute_result","execution_count":92,"data":{"text/plain":"'cladding'"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming the whole BOW corpus into a sparse matrix\n##\nbow_text = bow_transformer.transform(train['text'])","execution_count":93,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Shape of the matrix\n##\nbow_text.shape","execution_count":100,"outputs":[{"output_type":"execute_result","execution_count":100,"data":{"text/plain":"(7613, 6702)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#tfidf weighting\n##\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\ntfidf_transformer.fit(bow_text)","execution_count":101,"outputs":[{"output_type":"execute_result","execution_count":101,"data":{"text/plain":"TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming a single BOW\n##\nprint(tfidf_transformer.transform(bow_transformer.transform([train['text'][1230]])))","execution_count":102,"outputs":[{"output_type":"stream","text":"  (0, 3017)\t0.4700906051478796\n  (0, 2698)\t0.34633250839624347\n  (0, 2248)\t0.22818049172662036\n  (0, 2099)\t0.43932738517349695\n  (0, 1048)\t0.4700906051478796\n  (0, 325)\t0.43932738517349695\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Getting idf of a word\n##\ntfidf_transformer.idf_[bow_transformer.vocabulary_['cladding']]","execution_count":103,"outputs":[{"output_type":"execute_result","execution_count":103,"data":{"text/plain":"8.551449575822552"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Transforming the whole sparse matrix\n##\ntfidf_text = tfidf_transformer.transform(bow_text)","execution_count":104,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_text.shape","execution_count":105,"outputs":[{"output_type":"execute_result","execution_count":105,"data":{"text/plain":"(7613, 6702)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## keyword as a feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# building a sparse matrix of keywords\n##\nkword_cv = CountVectorizer()\nkword_cv.fit(train['keyword'])","execution_count":109,"outputs":[{"output_type":"execute_result","execution_count":109,"data":{"text/plain":"CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                tokenizer=None, vocabulary=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# keywords BOW\n##\nkword_bow = kword_cv.transform(train['keyword'])","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tfidf keyword weights\n##\ntfidf_transformer.fit(kword_bow)\n\nkword_tfidf = tfidf_transformer.transform(kword_bow)","execution_count":113,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Combining the two sparse matrices (text and keyword)"},{"metadata":{"trusted":true},"cell_type":"code","source":"from scipy.sparse import hstack\n\nfeatures = hstack([tfidf_text, kword_tfidf])","execution_count":115,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ML Algorithm Selection**"},{"metadata":{"trusted":true},"cell_type":"code","source":"train['target'].value_counts()","execution_count":116,"outputs":[{"output_type":"execute_result","execution_count":116,"data":{"text/plain":"0    4342\n1    3271\nName: target, dtype: int64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# So, a classification algorithm eg. Logistic regression, naive bayes","execution_count":117,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()","execution_count":118,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train test split\n##\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, train['target'], test_size=0.3, random_state=101)","execution_count":119,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imports for model evaluation\nfrom sklearn.metrics import confusion_matrix, classification_report","execution_count":120,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr.fit(X_train, y_train)\nlr_pred = lr.predict(X_test)\n\nnb.fit(X_train, y_train)\nnb_pred = nb.predict(X_test)\n\nrfc.fit(X_train, y_train)\nrfc_pred = rfc.predict(X_test)","execution_count":121,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Model evaluations\n##\n\nprint(\"---------------------------------\")\nprint(\"Logistic Regression\")\nprint(\"---------------------------------\")\nprint(confusion_matrix(y_test, lr_pred))\nprint(classification_report(y_test, lr_pred))\nprint(\"---------------------------------\")\nprint(\"Naive Bayes MultinomialNB\")\nprint(\"---------------------------------\")\nprint(confusion_matrix(y_test, nb_pred))\nprint(classification_report(y_test, nb_pred))\nprint(\"---------------------------------\")\nprint(\"RandomForestClassifier\")\nprint(\"---------------------------------\")\nprint(confusion_matrix(y_test, rfc_pred))\nprint(classification_report(y_test, rfc_pred))","execution_count":122,"outputs":[{"output_type":"stream","text":"---------------------------------\nLogistic Regression\n---------------------------------\n[[1160  170]\n [ 287  667]]\n              precision    recall  f1-score   support\n\n           0       0.80      0.87      0.84      1330\n           1       0.80      0.70      0.74       954\n\n    accuracy                           0.80      2284\n   macro avg       0.80      0.79      0.79      2284\nweighted avg       0.80      0.80      0.80      2284\n\n---------------------------------\nNaive Bayes MultinomialNB\n---------------------------------\n[[1111  219]\n [ 258  696]]\n              precision    recall  f1-score   support\n\n           0       0.81      0.84      0.82      1330\n           1       0.76      0.73      0.74       954\n\n    accuracy                           0.79      2284\n   macro avg       0.79      0.78      0.78      2284\nweighted avg       0.79      0.79      0.79      2284\n\n---------------------------------\nRandomForestClassifier\n---------------------------------\n[[1152  178]\n [ 320  634]]\n              precision    recall  f1-score   support\n\n           0       0.78      0.87      0.82      1330\n           1       0.78      0.66      0.72       954\n\n    accuracy                           0.78      2284\n   macro avg       0.78      0.77      0.77      2284\nweighted avg       0.78      0.78      0.78      2284\n\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Logistic regression did quite well than the other two","execution_count":61,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Building a data pipeline to predict on the test data.**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Combining the features in one helper function\n##\nfrom sklearn.compose import ColumnTransformer\n\ncoltrans = ColumnTransformer([\n    (\"text bow\", CountVectorizer(analyzer=text_process), 'text'),\n    (\"kword bow\", CountVectorizer(), 'keyword')],\n    remainder = 'drop'\n)","execution_count":123,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"coltrans.fit(train)","execution_count":124,"outputs":[{"output_type":"execute_result","execution_count":124,"data":{"text/plain":"ColumnTransformer(n_jobs=None, remainder='drop', sparse_threshold=0.3,\n                  transformer_weights=None,\n                  transformers=[('text bow',\n                                 CountVectorizer(analyzer=<function text_process at 0x7f25fb8ea1e0>,\n                                                 binary=False,\n                                                 decode_error='strict',\n                                                 dtype=<class 'numpy.int64'>,\n                                                 encoding='utf-8',\n                                                 input='content',\n                                                 lowercase=True, max_df=1.0,\n                                                 max_features=None, min_df=1,\n                                                 ngram_range=(1, 1),\n                                                 pr...\n                                ('kword bow',\n                                 CountVectorizer(analyzer='word', binary=False,\n                                                 decode_error='strict',\n                                                 dtype=<class 'numpy.int64'>,\n                                                 encoding='utf-8',\n                                                 input='content',\n                                                 lowercase=True, max_df=1.0,\n                                                 max_features=None, min_df=1,\n                                                 ngram_range=(1, 1),\n                                                 preprocessor=None,\n                                                 stop_words=None,\n                                                 strip_accents=None,\n                                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n                                                 tokenizer=None,\n                                                 vocabulary=None),\n                                 'keyword')],\n                  verbose=False)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# tfidf off of the BOW produced by transforming the features as below\n##\ntfidf_train = coltrans.transform(train)","execution_count":130,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(features.shape)\nprint(tfidf_train.shape)","execution_count":131,"outputs":[{"output_type":"stream","text":"(7613, 6941)\n(7613, 6941)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PIPELINE\n\nfrom sklearn.pipeline import Pipeline\n\npipe = Pipeline([\n    (\"coltrans\", ColumnTransformer([\n        (\"text bow\", CountVectorizer(analyzer=text_process), 'text'),\n        (\"kword bow\", CountVectorizer(), 'keyword')],\n        remainder = 'drop')),\n    (\"tfidf\", TfidfTransformer()),\n    (\"clf\", LogisticRegression())\n])\n\n# Designing pipe1 and pipe2 for naive bayes and random forest \n# They are similar to the pipeline above apart from the classifier\n\npipe1 = Pipeline([\n    (\"coltrans\", ColumnTransformer([\n        (\"text bow\", CountVectorizer(analyzer=text_process), 'text'),\n        (\"kword bow\", CountVectorizer(), 'keyword')],\n        remainder = 'drop')),\n    (\"tfidf\", TfidfTransformer()),\n    (\"clf\", MultinomialNB())\n])\n\npipe2 = Pipeline([\n    (\"coltrans\", ColumnTransformer([\n        (\"text bow\", CountVectorizer(analyzer=text_process), 'text'),\n        (\"kword bow\", CountVectorizer(), 'keyword')],\n        remainder = 'drop')),\n    (\"tfidf\", TfidfTransformer()),\n    (\"clf\", RandomForestClassifier())\n])\n","execution_count":149,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"text_train, text_test, label_train, label_test = train_test_split(train[['text', 'keyword']], train['target'],\n                                                                  test_size=0.3, random_state=101) ","execution_count":135,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe.fit(text_train, label_train)","execution_count":137,"outputs":[{"output_type":"execute_result","execution_count":137,"data":{"text/plain":"Pipeline(memory=None,\n         steps=[('coltrans',\n                 ColumnTransformer(n_jobs=None, remainder='drop',\n                                   sparse_threshold=0.3,\n                                   transformer_weights=None,\n                                   transformers=[('text bow',\n                                                  CountVectorizer(analyzer=<function text_process at 0x7f25fb8ea1e0>,\n                                                                  binary=False,\n                                                                  decode_error='strict',\n                                                                  dtype=<class 'numpy.int64'>,\n                                                                  encoding='utf-8',\n                                                                  input='content',\n                                                                  lowercase=True,\n                                                                  max_df=1.0,\n                                                                  max_fea...\n                                   verbose=False)),\n                ('tfidf',\n                 TfidfTransformer(norm='l2', smooth_idf=True,\n                                  sublinear_tf=False, use_idf=True)),\n                ('clf',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=100,\n                                    multi_class='auto', n_jobs=None,\n                                    penalty='l2', random_state=None,\n                                    solver='lbfgs', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_pred = pipe.predict(test)","execution_count":142,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pipe_pred.shape","execution_count":143,"outputs":[{"output_type":"execute_result","execution_count":143,"data":{"text/plain":"(3263,)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test.shape","execution_count":144,"outputs":[{"output_type":"execute_result","execution_count":144,"data":{"text/plain":"(3263, 4)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission3 = pd.DataFrame({\n    \"id\": test['id'],\n    \"target\": pipe_pred\n})","execution_count":145,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission.head()","execution_count":150,"outputs":[{"output_type":"execute_result","execution_count":150,"data":{"text/plain":"   id  target\n0   0       0\n1   2       0\n2   3       0\n3   9       0\n4  11       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission3","execution_count":147,"outputs":[{"output_type":"execute_result","execution_count":147,"data":{"text/plain":"         id  target\n0         0       1\n1         2       1\n2         3       1\n3         9       1\n4        11       1\n...     ...     ...\n3258  10861       1\n3259  10865       1\n3260  10868       1\n3261  10874       1\n3262  10875       1\n\n[3263 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>11</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3258</th>\n      <td>10861</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3259</th>\n      <td>10865</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3260</th>\n      <td>10868</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3261</th>\n      <td>10874</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3262</th>\n      <td>10875</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>3263 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission3.to_csv('submission3.csv', index=False)","execution_count":148,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# More submissions from pipe1 and pipe2\n##\n# pipe1\npipe1.fit(text_train, label_train)\n\n# Prediction on test data\npipe1_pred = pipe1.predict(test)\n\n# pipe2\npipe2.fit(text_train, label_train)\n\npipe2_pred = pipe2.predict(test)","execution_count":151,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission4 = pd.DataFrame({\n    \"id\": test['id'],\n    \"target\": pipe1_pred\n})\n\nsubmission5 = pd.DataFrame({\n    \"id\": test['id'],\n    \"target\": pipe2_pred\n})","execution_count":152,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission4.to_csv(\"submission4.csv\", index=False)\nsubmission5.to_csv(\"submission5.csv\", index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}